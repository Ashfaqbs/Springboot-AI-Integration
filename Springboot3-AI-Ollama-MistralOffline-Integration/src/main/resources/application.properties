spring.application.name=Springboot3-AI-Ollama-MistralOffline-Integration
#https://docs.spring.io/spring-ai/reference/api/clients/ollama-chat.html

spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.7

#In the context of AI models, particularly those used for generating text or responses, the "temperature" parameter controls the randomness of the output. A higher temperature value (e.g., 0.7) makes the output more random and creative, while a lower value (e.g., 0.2) makes it more deterministic and focused on the most likely predictions. 
#This parameter is crucial for balancing between creativity and coherence in generated content.

